Trainable params (LoRA): 5,242,880
Loaded train: 18200, val: 5226, test: 2574
Starting training..., iters: 3000
Iter 1: Val loss 4.515, Val took 20.779s
Iter 10: Train loss 0.965, Learning Rate 3.000e-05, It/sec 0.672, Tokens/sec 77.162, Trained Tokens 1148, Peak mem 19.767 GB
Iter 20: Train loss 0.191, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.882, Trained Tokens 2334, Peak mem 19.767 GB
Iter 30: Train loss 0.170, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 79.667, Trained Tokens 3501, Peak mem 19.767 GB
Iter 40: Train loss 0.142, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.308, Trained Tokens 4678, Peak mem 19.767 GB
Iter 50: Train loss 0.121, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.916, Trained Tokens 5849, Peak mem 19.767 GB
Iter 60: Train loss 0.117, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.189, Trained Tokens 7024, Peak mem 19.767 GB
Iter 70: Train loss 0.125, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.584, Trained Tokens 8190, Peak mem 19.767 GB
Iter 80: Train loss 0.111, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.670, Trained Tokens 9372, Peak mem 19.767 GB
Iter 90: Train loss 0.105, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.573, Trained Tokens 10538, Peak mem 19.767 GB
Iter 100: Val loss 0.088, Val took 20.911s
Iter 100: Train loss 0.090, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.042, Trained Tokens 11711, Peak mem 19.767 GB
Iter 100: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0000100_adapters.safetensors.
Iter 110: Train loss 0.102, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 80.347, Trained Tokens 12888, Peak mem 19.788 GB
Iter 120: Train loss 0.095, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.043, Trained Tokens 14061, Peak mem 19.788 GB
Iter 130: Train loss 0.113, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.972, Trained Tokens 15248, Peak mem 19.788 GB
Iter 140: Train loss 0.094, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.186, Trained Tokens 16438, Peak mem 19.788 GB
Iter 150: Train loss 0.088, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 80.402, Trained Tokens 17616, Peak mem 19.788 GB
Iter 160: Train loss 0.108, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.779, Trained Tokens 18786, Peak mem 19.788 GB
Iter 170: Train loss 0.086, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.423, Trained Tokens 19950, Peak mem 19.788 GB
Iter 180: Train loss 0.089, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.342, Trained Tokens 21128, Peak mem 19.788 GB
Iter 190: Train loss 0.089, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.868, Trained Tokens 22328, Peak mem 19.788 GB
Iter 200: Val loss 0.074, Val took 20.759s
Iter 200: Train loss 0.085, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.236, Trained Tokens 23489, Peak mem 19.788 GB
Iter 200: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0000200_adapters.safetensors.
Iter 210: Train loss 0.092, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.333, Trained Tokens 24681, Peak mem 19.788 GB
Iter 220: Train loss 0.083, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.096, Trained Tokens 25870, Peak mem 19.788 GB
Iter 230: Train loss 0.086, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.649, Trained Tokens 27067, Peak mem 20.322 GB
Iter 240: Train loss 0.083, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.254, Trained Tokens 28243, Peak mem 20.322 GB
Iter 250: Train loss 0.082, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 81.552, Trained Tokens 29453, Peak mem 20.322 GB
Iter 260: Train loss 0.084, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.612, Trained Tokens 30649, Peak mem 20.322 GB
Iter 270: Train loss 0.087, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.256, Trained Tokens 31825, Peak mem 20.322 GB
Iter 280: Train loss 0.071, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.577, Trained Tokens 33006, Peak mem 20.322 GB
Iter 290: Train loss 0.078, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.934, Trained Tokens 34192, Peak mem 20.322 GB
Iter 300: Val loss 0.065, Val took 21.059s
Iter 300: Train loss 0.084, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.391, Trained Tokens 35370, Peak mem 20.322 GB
Iter 300: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0000300_adapters.safetensors.
Iter 310: Train loss 0.083, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.774, Trained Tokens 36539, Peak mem 20.322 GB
Iter 320: Train loss 0.076, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 76.954, Trained Tokens 37681, Peak mem 20.322 GB
Iter 330: Train loss 0.060, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 77.694, Trained Tokens 38834, Peak mem 20.322 GB
Iter 340: Train loss 0.068, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.306, Trained Tokens 40011, Peak mem 20.322 GB
Iter 350: Train loss 0.071, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.573, Trained Tokens 41207, Peak mem 20.322 GB
Iter 360: Train loss 0.063, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 79.228, Trained Tokens 42368, Peak mem 20.322 GB
Iter 370: Train loss 0.075, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.446, Trained Tokens 43562, Peak mem 20.322 GB
Iter 380: Train loss 0.067, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.176, Trained Tokens 44737, Peak mem 20.322 GB
Iter 390: Train loss 0.066, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.105, Trained Tokens 45911, Peak mem 20.322 GB
Iter 400: Val loss 0.055, Val took 20.610s
Iter 400: Train loss 0.062, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.038, Trained Tokens 47084, Peak mem 20.322 GB
Iter 400: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0000400_adapters.safetensors.
Iter 410: Train loss 0.062, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.702, Trained Tokens 48252, Peak mem 20.322 GB
Iter 420: Train loss 0.070, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.643, Trained Tokens 49449, Peak mem 20.322 GB
Iter 430: Train loss 0.061, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.767, Trained Tokens 50648, Peak mem 20.322 GB
Iter 440: Train loss 0.066, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 76.898, Trained Tokens 51804, Peak mem 20.322 GB
Iter 450: Train loss 0.075, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.228, Trained Tokens 52995, Peak mem 20.322 GB
Iter 460: Train loss 0.049, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.453, Trained Tokens 54159, Peak mem 20.322 GB
Iter 470: Train loss 0.055, Learning Rate 3.000e-05, It/sec 0.691, Tokens/sec 81.170, Trained Tokens 55333, Peak mem 20.322 GB
Iter 480: Train loss 0.046, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 81.719, Trained Tokens 56546, Peak mem 20.322 GB
Iter 490: Train loss 0.059, Learning Rate 3.000e-05, It/sec 0.666, Tokens/sec 79.069, Trained Tokens 57733, Peak mem 20.342 GB
Iter 500: Val loss 0.053, Val took 21.056s
Iter 500: Train loss 0.063, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 79.023, Trained Tokens 58891, Peak mem 20.342 GB
Iter 500: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0000500_adapters.safetensors.
Iter 510: Train loss 0.056, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.779, Trained Tokens 60075, Peak mem 20.342 GB
Iter 520: Train loss 0.057, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 77.837, Trained Tokens 61230, Peak mem 20.342 GB
Iter 530: Train loss 0.046, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.849, Trained Tokens 62400, Peak mem 20.342 GB
Iter 540: Train loss 0.042, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.844, Trained Tokens 63570, Peak mem 20.342 GB
Iter 550: Train loss 0.042, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 79.510, Trained Tokens 64735, Peak mem 20.342 GB
Iter 560: Train loss 0.058, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.468, Trained Tokens 65915, Peak mem 20.342 GB
Iter 570: Train loss 0.055, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 81.566, Trained Tokens 67110, Peak mem 20.342 GB
Iter 580: Train loss 0.044, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.297, Trained Tokens 68302, Peak mem 20.342 GB
Iter 590: Train loss 0.040, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.308, Trained Tokens 69494, Peak mem 20.342 GB
Iter 600: Val loss 0.030, Val took 21.057s
Iter 600: Train loss 0.044, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.870, Trained Tokens 70694, Peak mem 20.342 GB
Iter 600: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0000600_adapters.safetensors.
Iter 610: Train loss 0.039, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.460, Trained Tokens 71873, Peak mem 20.342 GB
Iter 620: Train loss 0.055, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.302, Trained Tokens 73064, Peak mem 20.342 GB
Iter 630: Train loss 0.050, Learning Rate 3.000e-05, It/sec 0.673, Tokens/sec 78.453, Trained Tokens 74229, Peak mem 20.342 GB
Iter 640: Train loss 0.039, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.161, Trained Tokens 75419, Peak mem 20.342 GB
Iter 650: Train loss 0.036, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.309, Trained Tokens 76581, Peak mem 20.342 GB
Iter 660: Train loss 0.037, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 80.877, Trained Tokens 77766, Peak mem 20.342 GB
Iter 670: Train loss 0.030, Learning Rate 3.000e-05, It/sec 0.666, Tokens/sec 78.134, Trained Tokens 78940, Peak mem 20.342 GB
Iter 680: Train loss 0.040, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.530, Trained Tokens 80120, Peak mem 20.342 GB
Iter 690: Train loss 0.040, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 81.611, Trained Tokens 81316, Peak mem 20.342 GB
Iter 700: Val loss 0.028, Val took 20.761s
Iter 700: Train loss 0.048, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.703, Trained Tokens 82484, Peak mem 20.342 GB
Iter 700: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0000700_adapters.safetensors.
Iter 710: Train loss 0.039, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.147, Trained Tokens 83644, Peak mem 20.342 GB
Iter 720: Train loss 0.031, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.172, Trained Tokens 84834, Peak mem 20.342 GB
Iter 730: Train loss 0.027, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.427, Trained Tokens 85998, Peak mem 20.342 GB
Iter 740: Train loss 0.035, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.212, Trained Tokens 87159, Peak mem 20.342 GB
Iter 750: Train loss 0.039, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.561, Trained Tokens 88355, Peak mem 20.342 GB
Iter 760: Train loss 0.039, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 81.666, Trained Tokens 89567, Peak mem 20.342 GB
Iter 770: Train loss 0.039, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.170, Trained Tokens 90742, Peak mem 20.342 GB
Iter 780: Train loss 0.046, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.939, Trained Tokens 91929, Peak mem 20.342 GB
Iter 790: Train loss 0.042, Learning Rate 3.000e-05, It/sec 0.666, Tokens/sec 78.059, Trained Tokens 93101, Peak mem 20.342 GB
Iter 800: Val loss 0.039, Val took 20.908s
Iter 800: Train loss 0.040, Learning Rate 3.000e-05, It/sec 0.658, Tokens/sec 79.597, Trained Tokens 94311, Peak mem 20.342 GB
Iter 800: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0000800_adapters.safetensors.
Iter 810: Train loss 0.042, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.126, Trained Tokens 95486, Peak mem 20.342 GB
Iter 820: Train loss 0.037, Learning Rate 3.000e-05, It/sec 0.658, Tokens/sec 77.465, Trained Tokens 96664, Peak mem 20.342 GB
Iter 830: Train loss 0.037, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.687, Trained Tokens 97832, Peak mem 20.342 GB
Iter 840: Train loss 0.031, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.597, Trained Tokens 99014, Peak mem 20.342 GB
Iter 850: Train loss 0.026, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.639, Trained Tokens 100196, Peak mem 20.342 GB
Iter 860: Train loss 0.042, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 76.643, Trained Tokens 101348, Peak mem 20.342 GB
Iter 870: Train loss 0.040, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 77.837, Trained Tokens 102503, Peak mem 20.342 GB
Iter 880: Train loss 0.036, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.228, Trained Tokens 103694, Peak mem 20.342 GB
Iter 890: Train loss 0.034, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.382, Trained Tokens 104888, Peak mem 20.342 GB
Iter 900: Val loss 0.025, Val took 20.757s
Iter 900: Train loss 0.036, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 76.906, Trained Tokens 106044, Peak mem 20.342 GB
Iter 900: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0000900_adapters.safetensors.
Iter 910: Train loss 0.035, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.088, Trained Tokens 107233, Peak mem 20.342 GB
Iter 920: Train loss 0.039, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.828, Trained Tokens 108403, Peak mem 20.342 GB
Iter 930: Train loss 0.041, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.303, Trained Tokens 109580, Peak mem 20.342 GB
Iter 940: Train loss 0.030, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.996, Trained Tokens 110768, Peak mem 20.342 GB
Iter 950: Train loss 0.038, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 80.052, Trained Tokens 111941, Peak mem 20.342 GB
Iter 960: Train loss 0.043, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 81.721, Trained Tokens 113154, Peak mem 20.342 GB
Iter 970: Train loss 0.033, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.512, Trained Tokens 114334, Peak mem 20.342 GB
Iter 980: Train loss 0.041, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.370, Trained Tokens 115512, Peak mem 20.342 GB
Iter 990: Train loss 0.036, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.842, Trained Tokens 116712, Peak mem 20.342 GB
Iter 1000: Val loss 0.026, Val took 20.910s
Iter 1000: Train loss 0.028, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 77.143, Trained Tokens 117857, Peak mem 20.342 GB
Iter 1000: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0001000_adapters.safetensors.
Iter 1010: Train loss 0.033, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.740, Trained Tokens 119056, Peak mem 20.342 GB
Iter 1020: Train loss 0.039, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 79.225, Trained Tokens 120217, Peak mem 20.342 GB
Iter 1030: Train loss 0.037, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.502, Trained Tokens 121397, Peak mem 20.342 GB
Iter 1040: Train loss 0.033, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.698, Trained Tokens 122580, Peak mem 20.342 GB
Iter 1050: Train loss 0.033, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.629, Trained Tokens 123762, Peak mem 20.342 GB
Iter 1060: Train loss 0.024, Learning Rate 3.000e-05, It/sec 0.692, Tokens/sec 79.869, Trained Tokens 124917, Peak mem 20.342 GB
Iter 1070: Train loss 0.034, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 78.837, Trained Tokens 126072, Peak mem 20.342 GB
Iter 1080: Train loss 0.042, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.969, Trained Tokens 127244, Peak mem 20.342 GB
Iter 1090: Train loss 0.033, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.601, Trained Tokens 128411, Peak mem 20.342 GB
Iter 1100: Val loss 0.028, Val took 21.049s
Iter 1100: Train loss 0.042, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.100, Trained Tokens 129585, Peak mem 20.342 GB
Iter 1100: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0001100_adapters.safetensors.
Iter 1110: Train loss 0.052, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.832, Trained Tokens 130770, Peak mem 20.342 GB
Iter 1120: Train loss 0.042, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.154, Trained Tokens 131960, Peak mem 20.342 GB
Iter 1130: Train loss 0.037, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 82.856, Trained Tokens 133174, Peak mem 20.342 GB
Iter 1140: Train loss 0.035, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.371, Trained Tokens 134367, Peak mem 20.342 GB
Iter 1150: Train loss 0.034, Learning Rate 3.000e-05, It/sec 0.701, Tokens/sec 83.236, Trained Tokens 135555, Peak mem 20.342 GB
Iter 1160: Train loss 0.033, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.637, Trained Tokens 136722, Peak mem 20.342 GB
Iter 1170: Train loss 0.026, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.754, Trained Tokens 137906, Peak mem 20.342 GB
Iter 1180: Train loss 0.045, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.666, Trained Tokens 139103, Peak mem 20.342 GB
Iter 1190: Train loss 0.029, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.433, Trained Tokens 140267, Peak mem 20.342 GB
Iter 1200: Val loss 0.026, Val took 20.754s
Iter 1200: Train loss 0.032, Learning Rate 3.000e-05, It/sec 0.658, Tokens/sec 78.220, Trained Tokens 141456, Peak mem 20.342 GB
Iter 1200: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0001200_adapters.safetensors.
Iter 1210: Train loss 0.035, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.831, Trained Tokens 142656, Peak mem 20.342 GB
Iter 1220: Train loss 0.035, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.638, Trained Tokens 143823, Peak mem 20.342 GB
Iter 1230: Train loss 0.028, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 80.858, Trained Tokens 145007, Peak mem 20.342 GB
Iter 1240: Train loss 0.028, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 79.583, Trained Tokens 146173, Peak mem 20.342 GB
Iter 1250: Train loss 0.035, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.216, Trained Tokens 147334, Peak mem 20.342 GB
Iter 1260: Train loss 0.033, Learning Rate 3.000e-05, It/sec 0.675, Tokens/sec 80.209, Trained Tokens 148523, Peak mem 20.342 GB
Iter 1270: Train loss 0.022, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 79.517, Trained Tokens 149688, Peak mem 20.342 GB
Iter 1280: Train loss 0.031, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 80.270, Trained Tokens 150864, Peak mem 20.342 GB
Iter 1290: Train loss 0.029, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.828, Trained Tokens 152049, Peak mem 20.342 GB
Iter 1300: Val loss 0.018, Val took 20.904s
Iter 1300: Train loss 0.025, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.828, Trained Tokens 153219, Peak mem 20.342 GB
Iter 1300: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0001300_adapters.safetensors.
Iter 1310: Train loss 0.022, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.620, Trained Tokens 154386, Peak mem 20.342 GB
Iter 1320: Train loss 0.030, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.367, Trained Tokens 155564, Peak mem 20.342 GB
Iter 1330: Train loss 0.038, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 81.348, Trained Tokens 156756, Peak mem 20.342 GB
Iter 1340: Train loss 0.033, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 80.416, Trained Tokens 157965, Peak mem 20.342 GB
Iter 1350: Train loss 0.039, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.445, Trained Tokens 159159, Peak mem 20.342 GB
Iter 1360: Train loss 0.036, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.816, Trained Tokens 160359, Peak mem 20.342 GB
Iter 1370: Train loss 0.038, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.301, Trained Tokens 161536, Peak mem 20.342 GB
Iter 1380: Train loss 0.030, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.882, Trained Tokens 162707, Peak mem 20.342 GB
Iter 1390: Train loss 0.028, Learning Rate 3.000e-05, It/sec 0.666, Tokens/sec 79.002, Trained Tokens 163893, Peak mem 20.342 GB
Iter 1400: Val loss 0.024, Val took 20.898s
Iter 1400: Train loss 0.041, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.180, Trained Tokens 165084, Peak mem 20.342 GB
Iter 1400: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0001400_adapters.safetensors.
Iter 1410: Train loss 0.030, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 80.483, Trained Tokens 166263, Peak mem 20.342 GB
Iter 1420: Train loss 0.027, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.116, Trained Tokens 167437, Peak mem 20.342 GB
Iter 1430: Train loss 0.035, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 80.696, Trained Tokens 168650, Peak mem 20.342 GB
Iter 1440: Train loss 0.027, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 79.452, Trained Tokens 169814, Peak mem 20.342 GB
Iter 1450: Train loss 0.022, Learning Rate 3.000e-05, It/sec 0.673, Tokens/sec 78.635, Trained Tokens 170982, Peak mem 20.342 GB
Iter 1460: Train loss 0.028, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.842, Trained Tokens 172152, Peak mem 20.342 GB
Iter 1470: Train loss 0.022, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.438, Trained Tokens 173331, Peak mem 20.342 GB
Iter 1480: Train loss 0.017, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.175, Trained Tokens 174491, Peak mem 20.342 GB
Iter 1490: Train loss 0.039, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.172, Trained Tokens 175666, Peak mem 20.342 GB
Iter 1500: Val loss 0.023, Val took 20.756s
Iter 1500: Train loss 0.027, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 80.207, Trained Tokens 176841, Peak mem 20.342 GB
Iter 1500: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0001500_adapters.safetensors.
Iter 1510: Train loss 0.035, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.885, Trained Tokens 178027, Peak mem 20.342 GB
Iter 1520: Train loss 0.034, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.493, Trained Tokens 179192, Peak mem 20.342 GB
Iter 1530: Train loss 0.030, Learning Rate 3.000e-05, It/sec 0.673, Tokens/sec 78.855, Trained Tokens 180363, Peak mem 20.342 GB
Iter 1540: Train loss 0.024, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 79.903, Trained Tokens 181534, Peak mem 20.342 GB
Iter 1550: Train loss 0.031, Learning Rate 3.000e-05, It/sec 0.657, Tokens/sec 78.218, Trained Tokens 182725, Peak mem 20.342 GB
Iter 1560: Train loss 0.021, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.840, Trained Tokens 183910, Peak mem 20.342 GB
Iter 1570: Train loss 0.034, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 81.476, Trained Tokens 185119, Peak mem 20.342 GB
Iter 1580: Train loss 0.032, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.501, Trained Tokens 186300, Peak mem 20.342 GB
Iter 1590: Train loss 0.026, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.011, Trained Tokens 187488, Peak mem 20.342 GB
Iter 1600: Val loss 0.017, Val took 20.904s
Iter 1600: Train loss 0.025, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.643, Trained Tokens 188685, Peak mem 20.342 GB
Iter 1600: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0001600_adapters.safetensors.
Iter 1610: Train loss 0.023, Learning Rate 3.000e-05, It/sec 0.691, Tokens/sec 82.075, Trained Tokens 189872, Peak mem 20.342 GB
Iter 1620: Train loss 0.030, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 80.190, Trained Tokens 191047, Peak mem 20.342 GB
Iter 1630: Train loss 0.025, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 81.842, Trained Tokens 192246, Peak mem 20.342 GB
Iter 1640: Train loss 0.025, Learning Rate 3.000e-05, It/sec 0.658, Tokens/sec 77.360, Trained Tokens 193422, Peak mem 20.342 GB
Iter 1650: Train loss 0.025, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.825, Trained Tokens 194607, Peak mem 20.342 GB
Iter 1660: Train loss 0.036, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.783, Trained Tokens 195791, Peak mem 20.342 GB
Iter 1670: Train loss 0.026, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.034, Trained Tokens 196965, Peak mem 20.342 GB
Iter 1680: Train loss 0.030, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.770, Trained Tokens 198149, Peak mem 20.342 GB
Iter 1690: Train loss 0.026, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.177, Trained Tokens 199339, Peak mem 20.342 GB
Iter 1700: Val loss 0.017, Val took 20.750s
Iter 1700: Train loss 0.027, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.254, Trained Tokens 200530, Peak mem 20.342 GB
Iter 1700: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0001700_adapters.safetensors.
Iter 1710: Train loss 0.032, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.654, Trained Tokens 201712, Peak mem 20.342 GB
Iter 1720: Train loss 0.025, Learning Rate 3.000e-05, It/sec 0.673, Tokens/sec 77.584, Trained Tokens 202864, Peak mem 20.342 GB
Iter 1730: Train loss 0.020, Learning Rate 3.000e-05, It/sec 0.658, Tokens/sec 76.867, Trained Tokens 204033, Peak mem 20.342 GB
Iter 1740: Train loss 0.023, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.516, Trained Tokens 205213, Peak mem 20.342 GB
Iter 1750: Train loss 0.022, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.720, Trained Tokens 206396, Peak mem 20.342 GB
Iter 1760: Train loss 0.030, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.969, Trained Tokens 207568, Peak mem 20.342 GB
Iter 1770: Train loss 0.017, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 76.621, Trained Tokens 208720, Peak mem 20.342 GB
Iter 1780: Train loss 0.026, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.774, Trained Tokens 209904, Peak mem 20.342 GB
Iter 1790: Train loss 0.023, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.508, Trained Tokens 211084, Peak mem 20.342 GB
Iter 1800: Val loss 0.020, Val took 21.047s
Iter 1800: Train loss 0.018, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.281, Trained Tokens 212261, Peak mem 20.342 GB
Iter 1800: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0001800_adapters.safetensors.
Iter 1810: Train loss 0.016, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 80.060, Trained Tokens 213434, Peak mem 20.342 GB
Iter 1820: Train loss 0.022, Learning Rate 3.000e-05, It/sec 0.681, Tokens/sec 78.756, Trained Tokens 214590, Peak mem 20.342 GB
Iter 1830: Train loss 0.029, Learning Rate 3.000e-05, It/sec 0.673, Tokens/sec 77.988, Trained Tokens 215748, Peak mem 20.342 GB
Iter 1840: Train loss 0.024, Learning Rate 3.000e-05, It/sec 0.664, Tokens/sec 78.782, Trained Tokens 216935, Peak mem 20.342 GB
Iter 1850: Train loss 0.023, Learning Rate 3.000e-05, It/sec 0.650, Tokens/sec 77.945, Trained Tokens 218134, Peak mem 20.342 GB
Iter 1860: Train loss 0.027, Learning Rate 3.000e-05, It/sec 0.673, Tokens/sec 78.664, Trained Tokens 219302, Peak mem 20.342 GB
Iter 1870: Train loss 0.036, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.551, Trained Tokens 220468, Peak mem 20.342 GB
Iter 1880: Train loss 0.030, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.564, Trained Tokens 221649, Peak mem 20.342 GB
Iter 1890: Train loss 0.032, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.637, Trained Tokens 222831, Peak mem 20.342 GB
Iter 1900: Val loss 0.017, Val took 21.050s
Iter 1900: Train loss 0.022, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.367, Trained Tokens 223994, Peak mem 20.342 GB
Iter 1900: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0001900_adapters.safetensors.
Iter 1910: Train loss 0.025, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.350, Trained Tokens 225172, Peak mem 20.342 GB
Iter 1920: Train loss 0.016, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.091, Trained Tokens 226346, Peak mem 20.342 GB
Iter 1930: Train loss 0.023, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.035, Trained Tokens 227519, Peak mem 20.342 GB
Iter 1940: Train loss 0.024, Learning Rate 3.000e-05, It/sec 0.673, Tokens/sec 79.056, Trained Tokens 228693, Peak mem 20.342 GB
Iter 1950: Train loss 0.022, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.922, Trained Tokens 229864, Peak mem 20.342 GB
Iter 1960: Train loss 0.024, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 80.455, Trained Tokens 231043, Peak mem 20.342 GB
Iter 1970: Train loss 0.024, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.100, Trained Tokens 232232, Peak mem 20.342 GB
Iter 1980: Train loss 0.027, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.021, Trained Tokens 233420, Peak mem 20.342 GB
Iter 1990: Train loss 0.021, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 80.182, Trained Tokens 234595, Peak mem 20.342 GB
Iter 2000: Val loss 0.016, Val took 20.596s
Iter 2000: Train loss 0.023, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.238, Trained Tokens 235786, Peak mem 20.342 GB
Iter 2000: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0002000_adapters.safetensors.
Iter 2010: Train loss 0.021, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 76.827, Trained Tokens 236941, Peak mem 20.342 GB
Iter 2020: Train loss 0.029, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 81.362, Trained Tokens 238133, Peak mem 20.342 GB
Iter 2030: Train loss 0.018, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.762, Trained Tokens 239302, Peak mem 20.342 GB
Iter 2040: Train loss 0.025, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.315, Trained Tokens 240464, Peak mem 20.342 GB
Iter 2050: Train loss 0.029, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.168, Trained Tokens 241624, Peak mem 20.342 GB
Iter 2060: Train loss 0.022, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 80.003, Trained Tokens 242827, Peak mem 20.342 GB
Iter 2070: Train loss 0.022, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.890, Trained Tokens 244013, Peak mem 20.342 GB
Iter 2080: Train loss 0.024, Learning Rate 3.000e-05, It/sec 0.666, Tokens/sec 79.663, Trained Tokens 245210, Peak mem 20.342 GB
Iter 2090: Train loss 0.026, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.689, Trained Tokens 246393, Peak mem 20.342 GB
Iter 2100: Val loss 0.014, Val took 20.900s
Iter 2100: Train loss 0.017, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 80.090, Trained Tokens 247597, Peak mem 20.342 GB
Iter 2100: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0002100_adapters.safetensors.
Iter 2110: Train loss 0.024, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.576, Trained Tokens 248778, Peak mem 20.342 GB
Iter 2120: Train loss 0.021, Learning Rate 3.000e-05, It/sec 0.691, Tokens/sec 79.517, Trained Tokens 249928, Peak mem 20.342 GB
Iter 2130: Train loss 0.027, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.984, Trained Tokens 251130, Peak mem 20.342 GB
Iter 2140: Train loss 0.035, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.496, Trained Tokens 252325, Peak mem 20.342 GB
Iter 2150: Train loss 0.027, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.082, Trained Tokens 253499, Peak mem 20.342 GB
Iter 2160: Train loss 0.026, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 80.325, Trained Tokens 254676, Peak mem 20.342 GB
Iter 2170: Train loss 0.036, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 80.525, Trained Tokens 255856, Peak mem 20.342 GB
Iter 2180: Train loss 0.022, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.244, Trained Tokens 257047, Peak mem 20.342 GB
Iter 2190: Train loss 0.028, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.031, Trained Tokens 258235, Peak mem 20.342 GB
Iter 2200: Val loss 0.014, Val took 20.903s
Iter 2200: Train loss 0.018, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.754, Trained Tokens 259404, Peak mem 20.342 GB
Iter 2200: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0002200_adapters.safetensors.
Iter 2210: Train loss 0.035, Learning Rate 3.000e-05, It/sec 0.691, Tokens/sec 81.827, Trained Tokens 260588, Peak mem 20.342 GB
Iter 2220: Train loss 0.022, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 81.519, Trained Tokens 261798, Peak mem 20.342 GB
Iter 2230: Train loss 0.021, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.739, Trained Tokens 262967, Peak mem 20.342 GB
Iter 2240: Train loss 0.023, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.745, Trained Tokens 264166, Peak mem 20.342 GB
Iter 2250: Train loss 0.020, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.561, Trained Tokens 265362, Peak mem 20.342 GB
Iter 2260: Train loss 0.014, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.163, Trained Tokens 266552, Peak mem 20.342 GB
Iter 2270: Train loss 0.023, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.638, Trained Tokens 267734, Peak mem 20.342 GB
Iter 2280: Train loss 0.019, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.707, Trained Tokens 268917, Peak mem 20.342 GB
Iter 2290: Train loss 0.019, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.040, Trained Tokens 270091, Peak mem 20.342 GB
Iter 2300: Val loss 0.016, Val took 20.745s
Iter 2300: Train loss 0.026, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.757, Trained Tokens 271260, Peak mem 20.342 GB
Iter 2300: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0002300_adapters.safetensors.
Iter 2310: Train loss 0.030, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.925, Trained Tokens 272462, Peak mem 20.342 GB
Iter 2320: Train loss 0.024, Learning Rate 3.000e-05, It/sec 0.673, Tokens/sec 79.248, Trained Tokens 273639, Peak mem 20.342 GB
Iter 2330: Train loss 0.023, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.701, Trained Tokens 274822, Peak mem 20.342 GB
Iter 2340: Train loss 0.023, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 79.713, Trained Tokens 275990, Peak mem 20.342 GB
Iter 2350: Train loss 0.020, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 81.167, Trained Tokens 277179, Peak mem 20.342 GB
Iter 2360: Train loss 0.010, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.340, Trained Tokens 278342, Peak mem 20.342 GB
Iter 2370: Train loss 0.015, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.747, Trained Tokens 279526, Peak mem 20.342 GB
Iter 2380: Train loss 0.020, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.500, Trained Tokens 280721, Peak mem 20.342 GB
Iter 2390: Train loss 0.022, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.298, Trained Tokens 281898, Peak mem 20.342 GB
Iter 2400: Val loss 0.014, Val took 21.052s
Iter 2400: Train loss 0.018, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 76.698, Trained Tokens 283051, Peak mem 20.342 GB
Iter 2400: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0002400_adapters.safetensors.
Iter 2410: Train loss 0.016, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.759, Trained Tokens 284220, Peak mem 20.342 GB
Iter 2420: Train loss 0.014, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.875, Trained Tokens 285391, Peak mem 20.342 GB
Iter 2430: Train loss 0.018, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.292, Trained Tokens 286583, Peak mem 20.342 GB
Iter 2440: Train loss 0.016, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 80.999, Trained Tokens 287770, Peak mem 20.342 GB
Iter 2450: Train loss 0.012, Learning Rate 3.000e-05, It/sec 0.691, Tokens/sec 79.037, Trained Tokens 288913, Peak mem 20.342 GB
Iter 2460: Train loss 0.022, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.876, Trained Tokens 290084, Peak mem 20.342 GB
Iter 2470: Train loss 0.021, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.481, Trained Tokens 291264, Peak mem 20.342 GB
Iter 2480: Train loss 0.022, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.488, Trained Tokens 292444, Peak mem 20.342 GB
Iter 2490: Train loss 0.020, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 81.829, Trained Tokens 293643, Peak mem 20.342 GB
Iter 2500: Val loss 0.013, Val took 20.898s
Iter 2500: Train loss 0.023, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.513, Trained Tokens 294824, Peak mem 20.342 GB
Iter 2500: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0002500_adapters.safetensors.
Iter 2510: Train loss 0.015, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.485, Trained Tokens 295989, Peak mem 20.342 GB
Iter 2520: Train loss 0.018, Learning Rate 3.000e-05, It/sec 0.666, Tokens/sec 78.045, Trained Tokens 297161, Peak mem 20.342 GB
Iter 2530: Train loss 0.019, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.829, Trained Tokens 298346, Peak mem 20.342 GB
Iter 2540: Train loss 0.015, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.158, Trained Tokens 299521, Peak mem 20.342 GB
Iter 2550: Train loss 0.014, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 79.238, Trained Tokens 300682, Peak mem 20.342 GB
Iter 2560: Train loss 0.022, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.819, Trained Tokens 301867, Peak mem 20.342 GB
Iter 2570: Train loss 0.017, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.884, Trained Tokens 303053, Peak mem 20.342 GB
Iter 2580: Train loss 0.019, Learning Rate 3.000e-05, It/sec 0.692, Tokens/sec 81.600, Trained Tokens 304233, Peak mem 20.342 GB
Iter 2590: Train loss 0.020, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.499, Trained Tokens 305413, Peak mem 20.342 GB
Iter 2600: Val loss 0.007, Val took 20.746s
Iter 2600: Train loss 0.020, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.428, Trained Tokens 306592, Peak mem 20.342 GB
Iter 2600: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0002600_adapters.safetensors.
Iter 2610: Train loss 0.019, Learning Rate 3.000e-05, It/sec 0.692, Tokens/sec 82.086, Trained Tokens 307779, Peak mem 20.342 GB
Iter 2620: Train loss 0.029, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 82.601, Trained Tokens 308989, Peak mem 20.342 GB
Iter 2630: Train loss 0.035, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.582, Trained Tokens 310170, Peak mem 20.342 GB
Iter 2640: Train loss 0.033, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.425, Trained Tokens 311349, Peak mem 20.342 GB
Iter 2650: Train loss 0.023, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.320, Trained Tokens 312526, Peak mem 20.342 GB
Iter 2660: Train loss 0.016, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.308, Trained Tokens 313688, Peak mem 20.342 GB
Iter 2670: Train loss 0.018, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.959, Trained Tokens 314890, Peak mem 20.342 GB
Iter 2680: Train loss 0.014, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.364, Trained Tokens 316068, Peak mem 20.342 GB
Iter 2690: Train loss 0.020, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.144, Trained Tokens 317258, Peak mem 20.342 GB
Iter 2700: Val loss 0.008, Val took 20.435s
Iter 2700: Train loss 0.010, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.300, Trained Tokens 318435, Peak mem 20.342 GB
Iter 2700: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0002700_adapters.safetensors.
Iter 2710: Train loss 0.015, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.480, Trained Tokens 319600, Peak mem 20.342 GB
Iter 2720: Train loss 0.038, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 80.317, Trained Tokens 320792, Peak mem 20.342 GB
Iter 2730: Train loss 0.034, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 78.679, Trained Tokens 321945, Peak mem 20.342 GB
Iter 2740: Train loss 0.016, Learning Rate 3.000e-05, It/sec 0.666, Tokens/sec 78.257, Trained Tokens 323120, Peak mem 20.342 GB
Iter 2750: Train loss 0.013, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.346, Trained Tokens 324298, Peak mem 20.342 GB
Iter 2760: Train loss 0.014, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 80.716, Trained Tokens 325481, Peak mem 20.342 GB
Iter 2770: Train loss 0.019, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.213, Trained Tokens 326672, Peak mem 20.342 GB
Iter 2780: Train loss 0.009, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.016, Trained Tokens 327830, Peak mem 20.342 GB
Iter 2790: Train loss 0.015, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 78.489, Trained Tokens 329010, Peak mem 20.342 GB
Iter 2800: Val loss 0.011, Val took 20.744s
Iter 2800: Train loss 0.018, Learning Rate 3.000e-05, It/sec 0.682, Tokens/sec 80.988, Trained Tokens 330197, Peak mem 20.342 GB
Iter 2800: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0002800_adapters.safetensors.
Iter 2810: Train loss 0.022, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.233, Trained Tokens 331373, Peak mem 20.342 GB
Iter 2820: Train loss 0.012, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.705, Trained Tokens 332541, Peak mem 20.342 GB
Iter 2830: Train loss 0.014, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.433, Trained Tokens 333705, Peak mem 20.342 GB
Iter 2840: Train loss 0.018, Learning Rate 3.000e-05, It/sec 0.683, Tokens/sec 79.392, Trained Tokens 334868, Peak mem 20.342 GB
Iter 2850: Train loss 0.017, Learning Rate 3.000e-05, It/sec 0.658, Tokens/sec 78.929, Trained Tokens 336067, Peak mem 20.342 GB
Iter 2860: Train loss 0.012, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 77.101, Trained Tokens 337226, Peak mem 20.342 GB
Iter 2870: Train loss 0.013, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.368, Trained Tokens 338389, Peak mem 20.342 GB
Iter 2880: Train loss 0.016, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.295, Trained Tokens 339581, Peak mem 20.342 GB
Iter 2890: Train loss 0.021, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 79.287, Trained Tokens 340773, Peak mem 20.342 GB
Iter 2900: Val loss 0.008, Val took 21.048s
Iter 2900: Train loss 0.011, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.304, Trained Tokens 341950, Peak mem 20.342 GB
Iter 2900: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0002900_adapters.safetensors.
Iter 2910: Train loss 0.014, Learning Rate 3.000e-05, It/sec 0.673, Tokens/sec 80.164, Trained Tokens 343141, Peak mem 20.342 GB
Iter 2920: Train loss 0.015, Learning Rate 3.000e-05, It/sec 0.658, Tokens/sec 78.860, Trained Tokens 344340, Peak mem 20.342 GB
Iter 2930: Train loss 0.018, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.331, Trained Tokens 345503, Peak mem 20.342 GB
Iter 2940: Train loss 0.018, Learning Rate 3.000e-05, It/sec 0.658, Tokens/sec 78.203, Trained Tokens 346692, Peak mem 20.342 GB
Iter 2950: Train loss 0.010, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.303, Trained Tokens 347869, Peak mem 20.342 GB
Iter 2960: Train loss 0.018, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.705, Trained Tokens 349052, Peak mem 20.342 GB
Iter 2970: Train loss 0.009, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.037, Trained Tokens 350225, Peak mem 20.342 GB
Iter 2980: Train loss 0.015, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 79.635, Trained Tokens 351407, Peak mem 20.342 GB
Iter 2990: Train loss 0.009, Learning Rate 3.000e-05, It/sec 0.665, Tokens/sec 76.962, Trained Tokens 352564, Peak mem 20.342 GB
Iter 3000: Val loss 0.017, Val took 20.766s
Iter 3000: Train loss 0.011, Learning Rate 3.000e-05, It/sec 0.674, Tokens/sec 78.227, Trained Tokens 353725, Peak mem 20.342 GB
Iter 3000: Saved adapter weights to finetuned_model/adapters_dir_start_3/adapters.safetensors and finetuned_model/adapters_dir_start_3/0003000_adapters.safetensors.
Saved final weights to finetuned_model/adapters_dir_start_3/adapters.safetensors.
